{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ecb775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T06:38:07.683459Z",
     "start_time": "2022-08-31T06:38:07.674401Z"
    }
   },
   "source": [
    "<h2> Fine Tune Sentence Transformer Embeddings without Model Training </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9045231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:43:33.285186Z",
     "start_time": "2022-10-01T11:43:21.120961Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e5bc2",
   "metadata": {},
   "source": [
    "<b> Sentence Transformers:</b> <p>Sentence Transformers is a Python framework for state-of-the-art sentence, text embeddings. It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like semantic searchÂ , paraphrase mining.</p>\n",
    "\n",
    "<b> Paraphrase-MiniLM-L6-v2:</b> <p> is based on BERT with 6 Transformer Encoder Layers,it can handle 512 tokens and return dense vector representation with 384 features</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399056d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:43:33.317852Z",
     "start_time": "2022-10-01T11:43:33.291101Z"
    }
   },
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = tf.cast(tf.tile(tf.expand_dims(attention_mask, -1), [1, 1, token_embeddings.shape[-1]]), tf.float32)\n",
    "    return tf.math.reduce_sum(token_embeddings * input_mask_expanded, 1) / tf.math.maximum(tf.math.reduce_sum(input_mask_expanded, 1), 1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(tokenizer,texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
    "    \n",
    "    # Compute token embeddings\n",
    "    model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = tf.math.l2_normalize(embeddings, axis=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#cosine similarity Function\n",
    "def cosine_similarity(vector1,vector2):\n",
    "    return (1- cosine(vector1,vector2))\n",
    "\n",
    "#Function to find top_k similar words\n",
    "def most_similar(search_word,top_k=5):\n",
    "    search_idx = vocab_wrd2idx[search_word]\n",
    "    similarity_ls=[]\n",
    "    for word_idx,word_embed in enumerate(vocab_weights):\n",
    "        similarity_score = cosine_similarity(vocab_weights[search_idx],word_embed)\n",
    "        similarity_ls.append((word_idx,vocab_idx2wrd[word_idx],similarity_score))\n",
    "    \n",
    "    return pd.DataFrame(similarity_ls,columns=['word_index','word','score']).sort_values(by='score',ascending=False)[0:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15cc804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:43:39.883730Z",
     "start_time": "2022-10-01T11:43:33.321611Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ./sentence-transformer-paraphrase-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./sentence-transformer-paraphrase-MiniLM-L6-v2')\n",
    "model = TFAutoModel.from_pretrained('./sentence-transformer-paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a2cad7",
   "metadata": {},
   "source": [
    "<b> Model Architecture</b>\n",
    "\n",
    "<div style=\"overflow-y: scroll; height:400px;\">\n",
    "<pre>\n",
    "BertModel(\n",
    "  (embeddings): BertEmbeddings(\n",
    "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
    "    (position_embeddings): Embedding(512, 384)\n",
    "    (token_type_embeddings): Embedding(2, 384)\n",
    "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "    (dropout): Dropout(p=0.1, inplace=False)\n",
    "  )\n",
    "  (encoder): BertEncoder(\n",
    "    (layer): ModuleList(\n",
    "      (0): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (1): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (2): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (3): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (4): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (5): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (pooler): BertPooler(\n",
    "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "    (activation): Tanh()\n",
    "  )\n",
    ")\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44091072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T06:50:27.109337Z",
     "start_time": "2022-08-31T06:50:27.094311Z"
    }
   },
   "source": [
    "<h3> Objective: </h3>\n",
    "<ul>\n",
    "    <li> Add New Words to Vocabulary </li>\n",
    "    <li> Modify Existing word embeddings to adapt it for our use case</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639d5ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T06:52:41.248425Z",
     "start_time": "2022-08-31T06:52:41.235343Z"
    }
   },
   "source": [
    "<h2>1. Add New Words to Vocabulary </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db8de4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T06:45:11.492096Z",
     "start_time": "2022-08-31T06:45:11.469316Z"
    }
   },
   "source": [
    "<h4> Vocabulary Exploration </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2797a3c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:43:40.074347Z",
     "start_time": "2022-10-01T11:43:39.889134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('[PAD]', 0),\n",
       " ('[unused0]', 1),\n",
       " ('[unused1]', 2),\n",
       " ('[unused2]', 3),\n",
       " ('[unused3]', 4),\n",
       " ('[unused4]', 5),\n",
       " ('[unused5]', 6),\n",
       " ('[unused6]', 7),\n",
       " ('[unused7]', 8),\n",
       " ('[unused8]', 9),\n",
       " ('[unused9]', 10),\n",
       " ('[unused10]', 11),\n",
       " ('[unused11]', 12),\n",
       " ('[unused12]', 13),\n",
       " ('[unused13]', 14),\n",
       " ('[unused14]', 15),\n",
       " ('[unused15]', 16),\n",
       " ('[unused16]', 17),\n",
       " ('[unused17]', 18),\n",
       " ('[unused18]', 19),\n",
       " ('[unused19]', 20),\n",
       " ('[unused20]', 21),\n",
       " ('[unused21]', 22),\n",
       " ('[unused22]', 23),\n",
       " ('[unused23]', 24),\n",
       " ('[unused24]', 25),\n",
       " ('[unused25]', 26),\n",
       " ('[unused26]', 27),\n",
       " ('[unused27]', 28),\n",
       " ('[unused28]', 29),\n",
       " ('[unused29]', 30),\n",
       " ('[unused30]', 31),\n",
       " ('[unused31]', 32),\n",
       " ('[unused32]', 33),\n",
       " ('[unused33]', 34),\n",
       " ('[unused34]', 35),\n",
       " ('[unused35]', 36),\n",
       " ('[unused36]', 37),\n",
       " ('[unused37]', 38),\n",
       " ('[unused38]', 39),\n",
       " ('[unused39]', 40),\n",
       " ('[unused40]', 41),\n",
       " ('[unused41]', 42),\n",
       " ('[unused42]', 43),\n",
       " ('[unused43]', 44),\n",
       " ('[unused44]', 45),\n",
       " ('[unused45]', 46),\n",
       " ('[unused46]', 47),\n",
       " ('[unused47]', 48),\n",
       " ('[unused48]', 49),\n",
       " ('[unused49]', 50),\n",
       " ('[unused50]', 51),\n",
       " ('[unused51]', 52),\n",
       " ('[unused52]', 53),\n",
       " ('[unused53]', 54),\n",
       " ('[unused54]', 55),\n",
       " ('[unused55]', 56),\n",
       " ('[unused56]', 57),\n",
       " ('[unused57]', 58),\n",
       " ('[unused58]', 59),\n",
       " ('[unused59]', 60),\n",
       " ('[unused60]', 61),\n",
       " ('[unused61]', 62),\n",
       " ('[unused62]', 63),\n",
       " ('[unused63]', 64),\n",
       " ('[unused64]', 65),\n",
       " ('[unused65]', 66),\n",
       " ('[unused66]', 67),\n",
       " ('[unused67]', 68),\n",
       " ('[unused68]', 69),\n",
       " ('[unused69]', 70),\n",
       " ('[unused70]', 71),\n",
       " ('[unused71]', 72),\n",
       " ('[unused72]', 73),\n",
       " ('[unused73]', 74),\n",
       " ('[unused74]', 75),\n",
       " ('[unused75]', 76),\n",
       " ('[unused76]', 77),\n",
       " ('[unused77]', 78),\n",
       " ('[unused78]', 79),\n",
       " ('[unused79]', 80),\n",
       " ('[unused80]', 81),\n",
       " ('[unused81]', 82),\n",
       " ('[unused82]', 83),\n",
       " ('[unused83]', 84),\n",
       " ('[unused84]', 85),\n",
       " ('[unused85]', 86),\n",
       " ('[unused86]', 87),\n",
       " ('[unused87]', 88),\n",
       " ('[unused88]', 89),\n",
       " ('[unused89]', 90),\n",
       " ('[unused90]', 91),\n",
       " ('[unused91]', 92),\n",
       " ('[unused92]', 93),\n",
       " ('[unused93]', 94),\n",
       " ('[unused94]', 95),\n",
       " ('[unused95]', 96),\n",
       " ('[unused96]', 97),\n",
       " ('[unused97]', 98),\n",
       " ('[unused98]', 99),\n",
       " ('[UNK]', 100),\n",
       " ('[CLS]', 101),\n",
       " ('[SEP]', 102),\n",
       " ('[MASK]', 103),\n",
       " ('[unused99]', 104),\n",
       " ('[unused100]', 105),\n",
       " ('[unused101]', 106),\n",
       " ('[unused102]', 107),\n",
       " ('[unused103]', 108),\n",
       " ('[unused104]', 109),\n",
       " ('[unused105]', 110),\n",
       " ('[unused106]', 111),\n",
       " ('[unused107]', 112),\n",
       " ('[unused108]', 113),\n",
       " ('[unused109]', 114),\n",
       " ('[unused110]', 115),\n",
       " ('[unused111]', 116),\n",
       " ('[unused112]', 117),\n",
       " ('[unused113]', 118),\n",
       " ('[unused114]', 119),\n",
       " ('[unused115]', 120),\n",
       " ('[unused116]', 121),\n",
       " ('[unused117]', 122),\n",
       " ('[unused118]', 123),\n",
       " ('[unused119]', 124),\n",
       " ('[unused120]', 125),\n",
       " ('[unused121]', 126),\n",
       " ('[unused122]', 127),\n",
       " ('[unused123]', 128),\n",
       " ('[unused124]', 129),\n",
       " ('[unused125]', 130),\n",
       " ('[unused126]', 131),\n",
       " ('[unused127]', 132),\n",
       " ('[unused128]', 133),\n",
       " ('[unused129]', 134),\n",
       " ('[unused130]', 135),\n",
       " ('[unused131]', 136),\n",
       " ('[unused132]', 137),\n",
       " ('[unused133]', 138),\n",
       " ('[unused134]', 139),\n",
       " ('[unused135]', 140),\n",
       " ('[unused136]', 141),\n",
       " ('[unused137]', 142),\n",
       " ('[unused138]', 143),\n",
       " ('[unused139]', 144),\n",
       " ('[unused140]', 145),\n",
       " ('[unused141]', 146),\n",
       " ('[unused142]', 147),\n",
       " ('[unused143]', 148),\n",
       " ('[unused144]', 149),\n",
       " ('[unused145]', 150),\n",
       " ('[unused146]', 151),\n",
       " ('[unused147]', 152),\n",
       " ('[unused148]', 153),\n",
       " ('[unused149]', 154),\n",
       " ('[unused150]', 155),\n",
       " ('[unused151]', 156),\n",
       " ('[unused152]', 157),\n",
       " ('[unused153]', 158),\n",
       " ('[unused154]', 159),\n",
       " ('[unused155]', 160),\n",
       " ('[unused156]', 161),\n",
       " ('[unused157]', 162),\n",
       " ('[unused158]', 163),\n",
       " ('[unused159]', 164),\n",
       " ('[unused160]', 165),\n",
       " ('[unused161]', 166),\n",
       " ('[unused162]', 167),\n",
       " ('[unused163]', 168),\n",
       " ('[unused164]', 169),\n",
       " ('[unused165]', 170),\n",
       " ('[unused166]', 171),\n",
       " ('[unused167]', 172),\n",
       " ('[unused168]', 173),\n",
       " ('[unused169]', 174),\n",
       " ('[unused170]', 175),\n",
       " ('[unused171]', 176),\n",
       " ('[unused172]', 177),\n",
       " ('[unused173]', 178),\n",
       " ('[unused174]', 179),\n",
       " ('[unused175]', 180),\n",
       " ('[unused176]', 181),\n",
       " ('[unused177]', 182),\n",
       " ('[unused178]', 183),\n",
       " ('[unused179]', 184),\n",
       " ('[unused180]', 185),\n",
       " ('[unused181]', 186),\n",
       " ('[unused182]', 187),\n",
       " ('[unused183]', 188),\n",
       " ('[unused184]', 189),\n",
       " ('[unused185]', 190),\n",
       " ('[unused186]', 191),\n",
       " ('[unused187]', 192),\n",
       " ('[unused188]', 193),\n",
       " ('[unused189]', 194),\n",
       " ('[unused190]', 195),\n",
       " ('[unused191]', 196),\n",
       " ('[unused192]', 197),\n",
       " ('[unused193]', 198),\n",
       " ('[unused194]', 199),\n",
       " ('[unused195]', 200),\n",
       " ('[unused196]', 201),\n",
       " ('[unused197]', 202),\n",
       " ('[unused198]', 203),\n",
       " ('[unused199]', 204),\n",
       " ('[unused200]', 205),\n",
       " ('[unused201]', 206),\n",
       " ('[unused202]', 207),\n",
       " ('[unused203]', 208),\n",
       " ('[unused204]', 209),\n",
       " ('[unused205]', 210),\n",
       " ('[unused206]', 211),\n",
       " ('[unused207]', 212),\n",
       " ('[unused208]', 213),\n",
       " ('[unused209]', 214),\n",
       " ('[unused210]', 215),\n",
       " ('[unused211]', 216),\n",
       " ('[unused212]', 217),\n",
       " ('[unused213]', 218),\n",
       " ('[unused214]', 219),\n",
       " ('[unused215]', 220),\n",
       " ('[unused216]', 221),\n",
       " ('[unused217]', 222),\n",
       " ('[unused218]', 223),\n",
       " ('[unused219]', 224),\n",
       " ('[unused220]', 225),\n",
       " ('[unused221]', 226),\n",
       " ('[unused222]', 227),\n",
       " ('[unused223]', 228),\n",
       " ('[unused224]', 229),\n",
       " ('[unused225]', 230),\n",
       " ('[unused226]', 231),\n",
       " ('[unused227]', 232),\n",
       " ('[unused228]', 233),\n",
       " ('[unused229]', 234),\n",
       " ('[unused230]', 235),\n",
       " ('[unused231]', 236),\n",
       " ('[unused232]', 237),\n",
       " ('[unused233]', 238),\n",
       " ('[unused234]', 239),\n",
       " ('[unused235]', 240),\n",
       " ('[unused236]', 241),\n",
       " ('[unused237]', 242),\n",
       " ('[unused238]', 243),\n",
       " ('[unused239]', 244),\n",
       " ('[unused240]', 245),\n",
       " ('[unused241]', 246),\n",
       " ('[unused242]', 247),\n",
       " ('[unused243]', 248),\n",
       " ('[unused244]', 249),\n",
       " ('[unused245]', 250),\n",
       " ('[unused246]', 251),\n",
       " ('[unused247]', 252),\n",
       " ('[unused248]', 253),\n",
       " ('[unused249]', 254),\n",
       " ('[unused250]', 255),\n",
       " ('[unused251]', 256),\n",
       " ('[unused252]', 257),\n",
       " ('[unused253]', 258),\n",
       " ('[unused254]', 259),\n",
       " ('[unused255]', 260),\n",
       " ('[unused256]', 261),\n",
       " ('[unused257]', 262),\n",
       " ('[unused258]', 263),\n",
       " ('[unused259]', 264),\n",
       " ('[unused260]', 265),\n",
       " ('[unused261]', 266),\n",
       " ('[unused262]', 267),\n",
       " ('[unused263]', 268),\n",
       " ('[unused264]', 269),\n",
       " ('[unused265]', 270),\n",
       " ('[unused266]', 271),\n",
       " ('[unused267]', 272),\n",
       " ('[unused268]', 273),\n",
       " ('[unused269]', 274),\n",
       " ('[unused270]', 275),\n",
       " ('[unused271]', 276),\n",
       " ('[unused272]', 277),\n",
       " ('[unused273]', 278),\n",
       " ('[unused274]', 279),\n",
       " ('[unused275]', 280),\n",
       " ('[unused276]', 281),\n",
       " ('[unused277]', 282),\n",
       " ('[unused278]', 283),\n",
       " ('[unused279]', 284),\n",
       " ('[unused280]', 285),\n",
       " ('[unused281]', 286),\n",
       " ('[unused282]', 287),\n",
       " ('[unused283]', 288),\n",
       " ('[unused284]', 289),\n",
       " ('[unused285]', 290),\n",
       " ('[unused286]', 291),\n",
       " ('[unused287]', 292),\n",
       " ('[unused288]', 293),\n",
       " ('[unused289]', 294),\n",
       " ('[unused290]', 295),\n",
       " ('[unused291]', 296),\n",
       " ('[unused292]', 297),\n",
       " ('[unused293]', 298),\n",
       " ('[unused294]', 299),\n",
       " ('[unused295]', 300),\n",
       " ('[unused296]', 301),\n",
       " ('[unused297]', 302),\n",
       " ('[unused298]', 303),\n",
       " ('[unused299]', 304),\n",
       " ('[unused300]', 305),\n",
       " ('[unused301]', 306),\n",
       " ('[unused302]', 307),\n",
       " ('[unused303]', 308),\n",
       " ('[unused304]', 309),\n",
       " ('[unused305]', 310),\n",
       " ('[unused306]', 311),\n",
       " ('[unused307]', 312),\n",
       " ('[unused308]', 313),\n",
       " ('[unused309]', 314),\n",
       " ('[unused310]', 315),\n",
       " ('[unused311]', 316),\n",
       " ('[unused312]', 317),\n",
       " ('[unused313]', 318),\n",
       " ('[unused314]', 319),\n",
       " ('[unused315]', 320),\n",
       " ('[unused316]', 321),\n",
       " ('[unused317]', 322),\n",
       " ('[unused318]', 323),\n",
       " ('[unused319]', 324),\n",
       " ('[unused320]', 325),\n",
       " ('[unused321]', 326),\n",
       " ('[unused322]', 327),\n",
       " ('[unused323]', 328),\n",
       " ('[unused324]', 329),\n",
       " ('[unused325]', 330),\n",
       " ('[unused326]', 331),\n",
       " ('[unused327]', 332),\n",
       " ('[unused328]', 333),\n",
       " ('[unused329]', 334),\n",
       " ('[unused330]', 335),\n",
       " ('[unused331]', 336),\n",
       " ('[unused332]', 337),\n",
       " ('[unused333]', 338),\n",
       " ('[unused334]', 339),\n",
       " ('[unused335]', 340),\n",
       " ('[unused336]', 341),\n",
       " ('[unused337]', 342),\n",
       " ('[unused338]', 343),\n",
       " ('[unused339]', 344),\n",
       " ('[unused340]', 345),\n",
       " ('[unused341]', 346),\n",
       " ('[unused342]', 347),\n",
       " ('[unused343]', 348),\n",
       " ('[unused344]', 349),\n",
       " ('[unused345]', 350),\n",
       " ('[unused346]', 351),\n",
       " ('[unused347]', 352),\n",
       " ('[unused348]', 353),\n",
       " ('[unused349]', 354),\n",
       " ('[unused350]', 355),\n",
       " ('[unused351]', 356),\n",
       " ('[unused352]', 357),\n",
       " ('[unused353]', 358),\n",
       " ('[unused354]', 359),\n",
       " ('[unused355]', 360),\n",
       " ('[unused356]', 361),\n",
       " ('[unused357]', 362),\n",
       " ('[unused358]', 363),\n",
       " ('[unused359]', 364),\n",
       " ('[unused360]', 365),\n",
       " ('[unused361]', 366),\n",
       " ('[unused362]', 367),\n",
       " ('[unused363]', 368),\n",
       " ('[unused364]', 369),\n",
       " ('[unused365]', 370),\n",
       " ('[unused366]', 371),\n",
       " ('[unused367]', 372),\n",
       " ('[unused368]', 373),\n",
       " ('[unused369]', 374),\n",
       " ('[unused370]', 375),\n",
       " ('[unused371]', 376),\n",
       " ('[unused372]', 377),\n",
       " ('[unused373]', 378),\n",
       " ('[unused374]', 379),\n",
       " ('[unused375]', 380),\n",
       " ('[unused376]', 381),\n",
       " ('[unused377]', 382),\n",
       " ('[unused378]', 383),\n",
       " ('[unused379]', 384),\n",
       " ('[unused380]', 385),\n",
       " ('[unused381]', 386),\n",
       " ('[unused382]', 387),\n",
       " ('[unused383]', 388),\n",
       " ('[unused384]', 389),\n",
       " ('[unused385]', 390),\n",
       " ('[unused386]', 391),\n",
       " ('[unused387]', 392),\n",
       " ('[unused388]', 393),\n",
       " ('[unused389]', 394),\n",
       " ('[unused390]', 395),\n",
       " ('[unused391]', 396),\n",
       " ('[unused392]', 397),\n",
       " ('[unused393]', 398),\n",
       " ('[unused394]', 399),\n",
       " ('[unused395]', 400),\n",
       " ('[unused396]', 401),\n",
       " ('[unused397]', 402),\n",
       " ('[unused398]', 403),\n",
       " ('[unused399]', 404),\n",
       " ('[unused400]', 405),\n",
       " ('[unused401]', 406),\n",
       " ('[unused402]', 407),\n",
       " ('[unused403]', 408),\n",
       " ('[unused404]', 409),\n",
       " ('[unused405]', 410),\n",
       " ('[unused406]', 411),\n",
       " ('[unused407]', 412),\n",
       " ('[unused408]', 413),\n",
       " ('[unused409]', 414),\n",
       " ('[unused410]', 415),\n",
       " ('[unused411]', 416),\n",
       " ('[unused412]', 417),\n",
       " ('[unused413]', 418),\n",
       " ('[unused414]', 419),\n",
       " ('[unused415]', 420),\n",
       " ('[unused416]', 421),\n",
       " ('[unused417]', 422),\n",
       " ('[unused418]', 423),\n",
       " ('[unused419]', 424),\n",
       " ('[unused420]', 425),\n",
       " ('[unused421]', 426),\n",
       " ('[unused422]', 427),\n",
       " ('[unused423]', 428),\n",
       " ('[unused424]', 429),\n",
       " ('[unused425]', 430),\n",
       " ('[unused426]', 431),\n",
       " ('[unused427]', 432),\n",
       " ('[unused428]', 433),\n",
       " ('[unused429]', 434),\n",
       " ('[unused430]', 435),\n",
       " ('[unused431]', 436),\n",
       " ('[unused432]', 437),\n",
       " ('[unused433]', 438),\n",
       " ('[unused434]', 439),\n",
       " ('[unused435]', 440),\n",
       " ('[unused436]', 441),\n",
       " ('[unused437]', 442),\n",
       " ('[unused438]', 443),\n",
       " ('[unused439]', 444),\n",
       " ('[unused440]', 445),\n",
       " ('[unused441]', 446),\n",
       " ('[unused442]', 447),\n",
       " ('[unused443]', 448),\n",
       " ('[unused444]', 449),\n",
       " ('[unused445]', 450),\n",
       " ('[unused446]', 451),\n",
       " ('[unused447]', 452),\n",
       " ('[unused448]', 453),\n",
       " ('[unused449]', 454),\n",
       " ('[unused450]', 455),\n",
       " ('[unused451]', 456),\n",
       " ('[unused452]', 457),\n",
       " ('[unused453]', 458),\n",
       " ('[unused454]', 459),\n",
       " ('[unused455]', 460),\n",
       " ('[unused456]', 461),\n",
       " ('[unused457]', 462),\n",
       " ('[unused458]', 463),\n",
       " ('[unused459]', 464),\n",
       " ('[unused460]', 465),\n",
       " ('[unused461]', 466),\n",
       " ('[unused462]', 467),\n",
       " ('[unused463]', 468),\n",
       " ('[unused464]', 469),\n",
       " ('[unused465]', 470),\n",
       " ('[unused466]', 471),\n",
       " ('[unused467]', 472),\n",
       " ('[unused468]', 473),\n",
       " ('[unused469]', 474),\n",
       " ('[unused470]', 475),\n",
       " ('[unused471]', 476),\n",
       " ('[unused472]', 477),\n",
       " ('[unused473]', 478),\n",
       " ('[unused474]', 479),\n",
       " ('[unused475]', 480),\n",
       " ('[unused476]', 481),\n",
       " ('[unused477]', 482),\n",
       " ('[unused478]', 483),\n",
       " ('[unused479]', 484),\n",
       " ('[unused480]', 485),\n",
       " ('[unused481]', 486),\n",
       " ('[unused482]', 487),\n",
       " ('[unused483]', 488),\n",
       " ('[unused484]', 489),\n",
       " ('[unused485]', 490),\n",
       " ('[unused486]', 491),\n",
       " ('[unused487]', 492),\n",
       " ('[unused488]', 493),\n",
       " ('[unused489]', 494),\n",
       " ('[unused490]', 495),\n",
       " ('[unused491]', 496),\n",
       " ('[unused492]', 497),\n",
       " ('[unused493]', 498),\n",
       " ('[unused494]', 499),\n",
       " ('[unused495]', 500),\n",
       " ('[unused496]', 501),\n",
       " ('[unused497]', 502),\n",
       " ('[unused498]', 503),\n",
       " ('[unused499]', 504),\n",
       " ('[unused500]', 505),\n",
       " ('[unused501]', 506),\n",
       " ('[unused502]', 507),\n",
       " ('[unused503]', 508),\n",
       " ('[unused504]', 509),\n",
       " ('[unused505]', 510),\n",
       " ('[unused506]', 511),\n",
       " ('[unused507]', 512),\n",
       " ('[unused508]', 513),\n",
       " ('[unused509]', 514),\n",
       " ('[unused510]', 515),\n",
       " ('[unused511]', 516),\n",
       " ('[unused512]', 517),\n",
       " ('[unused513]', 518),\n",
       " ('[unused514]', 519),\n",
       " ('[unused515]', 520),\n",
       " ('[unused516]', 521),\n",
       " ('[unused517]', 522),\n",
       " ('[unused518]', 523),\n",
       " ('[unused519]', 524),\n",
       " ('[unused520]', 525),\n",
       " ('[unused521]', 526),\n",
       " ('[unused522]', 527),\n",
       " ('[unused523]', 528),\n",
       " ('[unused524]', 529),\n",
       " ('[unused525]', 530),\n",
       " ('[unused526]', 531),\n",
       " ('[unused527]', 532),\n",
       " ('[unused528]', 533),\n",
       " ('[unused529]', 534),\n",
       " ('[unused530]', 535),\n",
       " ('[unused531]', 536),\n",
       " ('[unused532]', 537),\n",
       " ('[unused533]', 538),\n",
       " ('[unused534]', 539),\n",
       " ('[unused535]', 540),\n",
       " ('[unused536]', 541),\n",
       " ('[unused537]', 542),\n",
       " ('[unused538]', 543),\n",
       " ('[unused539]', 544),\n",
       " ('[unused540]', 545),\n",
       " ('[unused541]', 546),\n",
       " ('[unused542]', 547),\n",
       " ('[unused543]', 548),\n",
       " ('[unused544]', 549),\n",
       " ('[unused545]', 550),\n",
       " ('[unused546]', 551),\n",
       " ('[unused547]', 552),\n",
       " ('[unused548]', 553),\n",
       " ('[unused549]', 554),\n",
       " ('[unused550]', 555),\n",
       " ('[unused551]', 556),\n",
       " ('[unused552]', 557),\n",
       " ('[unused553]', 558),\n",
       " ('[unused554]', 559),\n",
       " ('[unused555]', 560),\n",
       " ('[unused556]', 561),\n",
       " ('[unused557]', 562),\n",
       " ('[unused558]', 563),\n",
       " ('[unused559]', 564),\n",
       " ('[unused560]', 565),\n",
       " ('[unused561]', 566),\n",
       " ('[unused562]', 567),\n",
       " ('[unused563]', 568),\n",
       " ('[unused564]', 569),\n",
       " ('[unused565]', 570),\n",
       " ('[unused566]', 571),\n",
       " ('[unused567]', 572),\n",
       " ('[unused568]', 573),\n",
       " ('[unused569]', 574),\n",
       " ('[unused570]', 575),\n",
       " ('[unused571]', 576),\n",
       " ('[unused572]', 577),\n",
       " ('[unused573]', 578),\n",
       " ('[unused574]', 579),\n",
       " ('[unused575]', 580),\n",
       " ('[unused576]', 581),\n",
       " ('[unused577]', 582),\n",
       " ('[unused578]', 583),\n",
       " ('[unused579]', 584),\n",
       " ('[unused580]', 585),\n",
       " ('[unused581]', 586),\n",
       " ('[unused582]', 587),\n",
       " ('[unused583]', 588),\n",
       " ('[unused584]', 589),\n",
       " ('[unused585]', 590),\n",
       " ('[unused586]', 591),\n",
       " ('[unused587]', 592),\n",
       " ('[unused588]', 593),\n",
       " ('[unused589]', 594),\n",
       " ('[unused590]', 595),\n",
       " ('[unused591]', 596),\n",
       " ('[unused592]', 597),\n",
       " ('[unused593]', 598),\n",
       " ('[unused594]', 599),\n",
       " ('[unused595]', 600),\n",
       " ('[unused596]', 601),\n",
       " ('[unused597]', 602),\n",
       " ('[unused598]', 603),\n",
       " ('[unused599]', 604),\n",
       " ('[unused600]', 605),\n",
       " ('[unused601]', 606),\n",
       " ('[unused602]', 607),\n",
       " ('[unused603]', 608),\n",
       " ('[unused604]', 609),\n",
       " ('[unused605]', 610),\n",
       " ('[unused606]', 611),\n",
       " ('[unused607]', 612),\n",
       " ('[unused608]', 613),\n",
       " ('[unused609]', 614),\n",
       " ('[unused610]', 615),\n",
       " ('[unused611]', 616),\n",
       " ('[unused612]', 617),\n",
       " ('[unused613]', 618),\n",
       " ('[unused614]', 619),\n",
       " ('[unused615]', 620),\n",
       " ('[unused616]', 621),\n",
       " ('[unused617]', 622),\n",
       " ('[unused618]', 623),\n",
       " ('[unused619]', 624),\n",
       " ('[unused620]', 625),\n",
       " ('[unused621]', 626),\n",
       " ('[unused622]', 627),\n",
       " ('[unused623]', 628),\n",
       " ('[unused624]', 629),\n",
       " ('[unused625]', 630),\n",
       " ('[unused626]', 631),\n",
       " ('[unused627]', 632),\n",
       " ('[unused628]', 633),\n",
       " ('[unused629]', 634),\n",
       " ('[unused630]', 635),\n",
       " ('[unused631]', 636),\n",
       " ('[unused632]', 637),\n",
       " ('[unused633]', 638),\n",
       " ('[unused634]', 639),\n",
       " ('[unused635]', 640),\n",
       " ('[unused636]', 641),\n",
       " ('[unused637]', 642),\n",
       " ('[unused638]', 643),\n",
       " ('[unused639]', 644),\n",
       " ('[unused640]', 645),\n",
       " ('[unused641]', 646),\n",
       " ('[unused642]', 647),\n",
       " ('[unused643]', 648),\n",
       " ('[unused644]', 649),\n",
       " ('[unused645]', 650),\n",
       " ('[unused646]', 651),\n",
       " ('[unused647]', 652),\n",
       " ('[unused648]', 653),\n",
       " ('[unused649]', 654),\n",
       " ('[unused650]', 655),\n",
       " ('[unused651]', 656),\n",
       " ('[unused652]', 657),\n",
       " ('[unused653]', 658),\n",
       " ('[unused654]', 659),\n",
       " ('[unused655]', 660),\n",
       " ('[unused656]', 661),\n",
       " ('[unused657]', 662),\n",
       " ('[unused658]', 663),\n",
       " ('[unused659]', 664),\n",
       " ('[unused660]', 665),\n",
       " ('[unused661]', 666),\n",
       " ('[unused662]', 667),\n",
       " ('[unused663]', 668),\n",
       " ('[unused664]', 669),\n",
       " ('[unused665]', 670),\n",
       " ('[unused666]', 671),\n",
       " ('[unused667]', 672),\n",
       " ('[unused668]', 673),\n",
       " ('[unused669]', 674),\n",
       " ('[unused670]', 675),\n",
       " ('[unused671]', 676),\n",
       " ('[unused672]', 677),\n",
       " ('[unused673]', 678),\n",
       " ('[unused674]', 679),\n",
       " ('[unused675]', 680),\n",
       " ('[unused676]', 681),\n",
       " ('[unused677]', 682),\n",
       " ('[unused678]', 683),\n",
       " ('[unused679]', 684),\n",
       " ('[unused680]', 685),\n",
       " ('[unused681]', 686),\n",
       " ('[unused682]', 687),\n",
       " ('[unused683]', 688),\n",
       " ('[unused684]', 689),\n",
       " ('[unused685]', 690),\n",
       " ('[unused686]', 691),\n",
       " ('[unused687]', 692),\n",
       " ('[unused688]', 693),\n",
       " ('[unused689]', 694),\n",
       " ('[unused690]', 695),\n",
       " ('[unused691]', 696),\n",
       " ('[unused692]', 697),\n",
       " ('[unused693]', 698),\n",
       " ('[unused694]', 699),\n",
       " ('[unused695]', 700),\n",
       " ('[unused696]', 701),\n",
       " ('[unused697]', 702),\n",
       " ('[unused698]', 703),\n",
       " ('[unused699]', 704),\n",
       " ('[unused700]', 705),\n",
       " ('[unused701]', 706),\n",
       " ('[unused702]', 707),\n",
       " ('[unused703]', 708),\n",
       " ('[unused704]', 709),\n",
       " ('[unused705]', 710),\n",
       " ('[unused706]', 711),\n",
       " ('[unused707]', 712),\n",
       " ('[unused708]', 713),\n",
       " ('[unused709]', 714),\n",
       " ('[unused710]', 715),\n",
       " ('[unused711]', 716),\n",
       " ('[unused712]', 717),\n",
       " ('[unused713]', 718),\n",
       " ('[unused714]', 719),\n",
       " ('[unused715]', 720),\n",
       " ('[unused716]', 721),\n",
       " ('[unused717]', 722),\n",
       " ('[unused718]', 723),\n",
       " ('[unused719]', 724),\n",
       " ('[unused720]', 725),\n",
       " ('[unused721]', 726),\n",
       " ('[unused722]', 727),\n",
       " ('[unused723]', 728),\n",
       " ('[unused724]', 729),\n",
       " ('[unused725]', 730),\n",
       " ('[unused726]', 731),\n",
       " ('[unused727]', 732),\n",
       " ('[unused728]', 733),\n",
       " ('[unused729]', 734),\n",
       " ('[unused730]', 735),\n",
       " ('[unused731]', 736),\n",
       " ('[unused732]', 737),\n",
       " ('[unused733]', 738),\n",
       " ('[unused734]', 739),\n",
       " ('[unused735]', 740),\n",
       " ('[unused736]', 741),\n",
       " ('[unused737]', 742),\n",
       " ('[unused738]', 743),\n",
       " ('[unused739]', 744),\n",
       " ('[unused740]', 745),\n",
       " ('[unused741]', 746),\n",
       " ('[unused742]', 747),\n",
       " ('[unused743]', 748),\n",
       " ('[unused744]', 749),\n",
       " ('[unused745]', 750),\n",
       " ('[unused746]', 751),\n",
       " ('[unused747]', 752),\n",
       " ('[unused748]', 753),\n",
       " ('[unused749]', 754),\n",
       " ('[unused750]', 755),\n",
       " ('[unused751]', 756),\n",
       " ('[unused752]', 757),\n",
       " ('[unused753]', 758),\n",
       " ('[unused754]', 759),\n",
       " ('[unused755]', 760),\n",
       " ('[unused756]', 761),\n",
       " ('[unused757]', 762),\n",
       " ('[unused758]', 763),\n",
       " ('[unused759]', 764),\n",
       " ('[unused760]', 765),\n",
       " ('[unused761]', 766),\n",
       " ('[unused762]', 767),\n",
       " ('[unused763]', 768),\n",
       " ('[unused764]', 769),\n",
       " ('[unused765]', 770),\n",
       " ('[unused766]', 771),\n",
       " ('[unused767]', 772),\n",
       " ('[unused768]', 773),\n",
       " ('[unused769]', 774),\n",
       " ('[unused770]', 775),\n",
       " ('[unused771]', 776),\n",
       " ('[unused772]', 777),\n",
       " ('[unused773]', 778),\n",
       " ('[unused774]', 779),\n",
       " ('[unused775]', 780),\n",
       " ('[unused776]', 781),\n",
       " ('[unused777]', 782),\n",
       " ('[unused778]', 783),\n",
       " ('[unused779]', 784),\n",
       " ('[unused780]', 785),\n",
       " ('[unused781]', 786),\n",
       " ('[unused782]', 787),\n",
       " ('[unused783]', 788),\n",
       " ('[unused784]', 789),\n",
       " ('[unused785]', 790),\n",
       " ('[unused786]', 791),\n",
       " ('[unused787]', 792),\n",
       " ('[unused788]', 793),\n",
       " ('[unused789]', 794),\n",
       " ('[unused790]', 795),\n",
       " ('[unused791]', 796),\n",
       " ('[unused792]', 797),\n",
       " ('[unused793]', 798),\n",
       " ('[unused794]', 799),\n",
       " ('[unused795]', 800),\n",
       " ('[unused796]', 801),\n",
       " ('[unused797]', 802),\n",
       " ('[unused798]', 803),\n",
       " ('[unused799]', 804),\n",
       " ('[unused800]', 805),\n",
       " ('[unused801]', 806),\n",
       " ('[unused802]', 807),\n",
       " ('[unused803]', 808),\n",
       " ('[unused804]', 809),\n",
       " ('[unused805]', 810),\n",
       " ('[unused806]', 811),\n",
       " ('[unused807]', 812),\n",
       " ('[unused808]', 813),\n",
       " ('[unused809]', 814),\n",
       " ('[unused810]', 815),\n",
       " ('[unused811]', 816),\n",
       " ('[unused812]', 817),\n",
       " ('[unused813]', 818),\n",
       " ('[unused814]', 819),\n",
       " ('[unused815]', 820),\n",
       " ('[unused816]', 821),\n",
       " ('[unused817]', 822),\n",
       " ('[unused818]', 823),\n",
       " ('[unused819]', 824),\n",
       " ('[unused820]', 825),\n",
       " ('[unused821]', 826),\n",
       " ('[unused822]', 827),\n",
       " ('[unused823]', 828),\n",
       " ('[unused824]', 829),\n",
       " ('[unused825]', 830),\n",
       " ('[unused826]', 831),\n",
       " ('[unused827]', 832),\n",
       " ('[unused828]', 833),\n",
       " ('[unused829]', 834),\n",
       " ('[unused830]', 835),\n",
       " ('[unused831]', 836),\n",
       " ('[unused832]', 837),\n",
       " ('[unused833]', 838),\n",
       " ('[unused834]', 839),\n",
       " ('[unused835]', 840),\n",
       " ('[unused836]', 841),\n",
       " ('[unused837]', 842),\n",
       " ('[unused838]', 843),\n",
       " ('[unused839]', 844),\n",
       " ('[unused840]', 845),\n",
       " ('[unused841]', 846),\n",
       " ('[unused842]', 847),\n",
       " ('[unused843]', 848),\n",
       " ('[unused844]', 849),\n",
       " ('[unused845]', 850),\n",
       " ('[unused846]', 851),\n",
       " ('[unused847]', 852),\n",
       " ('[unused848]', 853),\n",
       " ('[unused849]', 854),\n",
       " ('[unused850]', 855),\n",
       " ('[unused851]', 856),\n",
       " ('[unused852]', 857),\n",
       " ('[unused853]', 858),\n",
       " ('[unused854]', 859),\n",
       " ('[unused855]', 860),\n",
       " ('[unused856]', 861),\n",
       " ('[unused857]', 862),\n",
       " ('[unused858]', 863),\n",
       " ('[unused859]', 864),\n",
       " ('[unused860]', 865),\n",
       " ('[unused861]', 866),\n",
       " ('[unused862]', 867),\n",
       " ('[unused863]', 868),\n",
       " ('[unused864]', 869),\n",
       " ('[unused865]', 870),\n",
       " ('[unused866]', 871),\n",
       " ('[unused867]', 872),\n",
       " ('[unused868]', 873),\n",
       " ('[unused869]', 874),\n",
       " ('[unused870]', 875),\n",
       " ('[unused871]', 876),\n",
       " ('[unused872]', 877),\n",
       " ('[unused873]', 878),\n",
       " ('[unused874]', 879),\n",
       " ('[unused875]', 880),\n",
       " ('[unused876]', 881),\n",
       " ('[unused877]', 882),\n",
       " ('[unused878]', 883),\n",
       " ('[unused879]', 884),\n",
       " ('[unused880]', 885),\n",
       " ('[unused881]', 886),\n",
       " ('[unused882]', 887),\n",
       " ('[unused883]', 888),\n",
       " ('[unused884]', 889),\n",
       " ('[unused885]', 890),\n",
       " ('[unused886]', 891),\n",
       " ('[unused887]', 892),\n",
       " ('[unused888]', 893),\n",
       " ('[unused889]', 894),\n",
       " ('[unused890]', 895),\n",
       " ('[unused891]', 896),\n",
       " ('[unused892]', 897),\n",
       " ('[unused893]', 898),\n",
       " ('[unused894]', 899),\n",
       " ('[unused895]', 900),\n",
       " ('[unused896]', 901),\n",
       " ('[unused897]', 902),\n",
       " ('[unused898]', 903),\n",
       " ('[unused899]', 904),\n",
       " ('[unused900]', 905),\n",
       " ('[unused901]', 906),\n",
       " ('[unused902]', 907),\n",
       " ('[unused903]', 908),\n",
       " ('[unused904]', 909),\n",
       " ('[unused905]', 910),\n",
       " ('[unused906]', 911),\n",
       " ('[unused907]', 912),\n",
       " ('[unused908]', 913),\n",
       " ('[unused909]', 914),\n",
       " ('[unused910]', 915),\n",
       " ('[unused911]', 916),\n",
       " ('[unused912]', 917),\n",
       " ('[unused913]', 918),\n",
       " ('[unused914]', 919),\n",
       " ('[unused915]', 920),\n",
       " ('[unused916]', 921),\n",
       " ('[unused917]', 922),\n",
       " ('[unused918]', 923),\n",
       " ('[unused919]', 924),\n",
       " ('[unused920]', 925),\n",
       " ('[unused921]', 926),\n",
       " ('[unused922]', 927),\n",
       " ('[unused923]', 928),\n",
       " ('[unused924]', 929),\n",
       " ('[unused925]', 930),\n",
       " ('[unused926]', 931),\n",
       " ('[unused927]', 932),\n",
       " ('[unused928]', 933),\n",
       " ('[unused929]', 934),\n",
       " ('[unused930]', 935),\n",
       " ('[unused931]', 936),\n",
       " ('[unused932]', 937),\n",
       " ('[unused933]', 938),\n",
       " ('[unused934]', 939),\n",
       " ('[unused935]', 940),\n",
       " ('[unused936]', 941),\n",
       " ('[unused937]', 942),\n",
       " ('[unused938]', 943),\n",
       " ('[unused939]', 944),\n",
       " ('[unused940]', 945),\n",
       " ('[unused941]', 946),\n",
       " ('[unused942]', 947),\n",
       " ('[unused943]', 948),\n",
       " ('[unused944]', 949),\n",
       " ('[unused945]', 950),\n",
       " ('[unused946]', 951),\n",
       " ('[unused947]', 952),\n",
       " ('[unused948]', 953),\n",
       " ('[unused949]', 954),\n",
       " ('[unused950]', 955),\n",
       " ('[unused951]', 956),\n",
       " ('[unused952]', 957),\n",
       " ('[unused953]', 958),\n",
       " ('[unused954]', 959),\n",
       " ('[unused955]', 960),\n",
       " ('[unused956]', 961),\n",
       " ('[unused957]', 962),\n",
       " ('[unused958]', 963),\n",
       " ('[unused959]', 964),\n",
       " ('[unused960]', 965),\n",
       " ('[unused961]', 966),\n",
       " ('[unused962]', 967),\n",
       " ('[unused963]', 968),\n",
       " ('[unused964]', 969),\n",
       " ('[unused965]', 970),\n",
       " ('[unused966]', 971),\n",
       " ('[unused967]', 972),\n",
       " ('[unused968]', 973),\n",
       " ('[unused969]', 974),\n",
       " ('[unused970]', 975),\n",
       " ('[unused971]', 976),\n",
       " ('[unused972]', 977),\n",
       " ('[unused973]', 978),\n",
       " ('[unused974]', 979),\n",
       " ('[unused975]', 980),\n",
       " ('[unused976]', 981),\n",
       " ('[unused977]', 982),\n",
       " ('[unused978]', 983),\n",
       " ('[unused979]', 984),\n",
       " ('[unused980]', 985),\n",
       " ('[unused981]', 986),\n",
       " ('[unused982]', 987),\n",
       " ('[unused983]', 988),\n",
       " ('[unused984]', 989),\n",
       " ('[unused985]', 990),\n",
       " ('[unused986]', 991),\n",
       " ('[unused987]', 992),\n",
       " ('[unused988]', 993),\n",
       " ('[unused989]', 994),\n",
       " ('[unused990]', 995),\n",
       " ('[unused991]', 996),\n",
       " ('[unused992]', 997),\n",
       " ('[unused993]', 998),\n",
       " ('!', 999),\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_wrd2idx = tokenizer.vocab\n",
    "vocab_idx2wrd = {v:k for k,v in vocab_wrd2idx.items()}\n",
    "\n",
    "print(len(vocab_wrd2idx))\n",
    "\n",
    "sorted(vocab_wrd2idx.items(),key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11364c86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T06:50:49.654741Z",
     "start_time": "2022-08-31T06:50:49.627778Z"
    }
   },
   "source": [
    "<p> We have 30,522 tokens in our vocab for paraphrase-MiniLM-l6-v2 model.\n",
    "It has 993 tokens which are not used, represented by [unusedXXX], \n",
    "These are the token which we can replace to add new tokens to our vocab.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92a03fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:43:40.323167Z",
     "start_time": "2022-10-01T11:43:40.079778Z"
    }
   },
   "outputs": [],
   "source": [
    "#extracting model weights\n",
    "model_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe77b7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:43:40.338369Z",
     "start_time": "2022-10-01T11:43:40.327057Z"
    }
   },
   "outputs": [],
   "source": [
    "#getting vocab weights\n",
    "vocab_weights = model_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af736d6a",
   "metadata": {},
   "source": [
    "<p>\n",
    "Suppose you have trained your model or you are using a pretrained model but what you observe is that some important words are missing from the vocabulary, and you don't want to retrain the model with new data,\n",
    "you can follow the below method to add new word/tokens to your model\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f028aa",
   "metadata": {},
   "source": [
    "<h4> Code for adding words </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ccb7ed1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:43:40.372252Z",
     "start_time": "2022-10-01T11:43:40.343982Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_average_of_vectors(word_wghts):\n",
    "    \n",
    "    vector = np.zeros(384, dtype=np.float64)\n",
    "    \n",
    "    for key,value in word_wghts.items():\n",
    "        vector += (1 - value) * vocab_weights[vocab_wrd2idx[key]]\n",
    "    \n",
    "    vector = normalize(vector.reshape(1,-1))[0]\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def add_word(word_to_replace,word_to_add,word_wghts):\n",
    "    \n",
    "    #editing the dictionary \n",
    "    vocab_index = vocab_wrd2idx[word_to_replace]\n",
    "    vocab_wrd2idx[word_to_add] = vocab_index\n",
    "    del vocab_wrd2idx[vocab_idx2wrd[vocab_index]]\n",
    "    vocab_idx2wrd[vocab_index] = word_to_add\n",
    "    \n",
    "    #add the vector\n",
    "    vocab_weights[vocab_index] = weighted_average_of_vectors(word_wghts)\n",
    "    \n",
    "    #settings weights\n",
    "    model_weights[0] = vocab_weights\n",
    "    model.set_weights(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69d609b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:43:40.488592Z",
     "start_time": "2022-10-01T11:43:40.377413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, True]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if words are in vocab\n",
    "\n",
    "words_to_add = [\"stmt\",\"ftp\",'http']\n",
    "\n",
    "[wrd in tokenizer.vocab for wrd in words_to_add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433b2da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T09:50:22.559033Z",
     "start_time": "2022-08-31T09:50:22.548236Z"
    }
   },
   "source": [
    "<b>Result:</b> only http is present in the vocab. ftp and stmt (short for stament) is not present (you can always replace the word at preprocessing,this is just for demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad8862e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:43:40.835288Z",
     "start_time": "2022-10-01T11:43:40.495278Z"
    }
   },
   "outputs": [],
   "source": [
    "#defining weights for new words\n",
    "\n",
    "ftp_wghts = {\n",
    "    'file':0.3,\n",
    "    'transfer':0.3,\n",
    "    'protocol':0.4,\n",
    "}\n",
    "\n",
    "stmt_wghts = {\n",
    "    'statement':0.5,\n",
    "    'bill':0.5\n",
    "}\n",
    "\n",
    "#adding words\n",
    "add_word('[unused900]','ftp',ftp_wghts)\n",
    "add_word('[unused902]','stmt',stmt_wghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3617e7eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:43:45.015033Z",
     "start_time": "2022-10-01T11:43:40.842581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_index</th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>907</td>\n",
       "      <td>stmt</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>3021</td>\n",
       "      <td>bill</td>\n",
       "      <td>0.761461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>4861</td>\n",
       "      <td>statement</td>\n",
       "      <td>0.739249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8236</th>\n",
       "      <td>8236</td>\n",
       "      <td>bills</td>\n",
       "      <td>0.520205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>8635</td>\n",
       "      <td>statements</td>\n",
       "      <td>0.465498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>3661</td>\n",
       "      <td>letter</td>\n",
       "      <td>0.393868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>6094</td>\n",
       "      <td>legislation</td>\n",
       "      <td>0.375259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>2552</td>\n",
       "      <td>act</td>\n",
       "      <td>0.331694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>2928</td>\n",
       "      <td>mark</td>\n",
       "      <td>0.313210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>3189</td>\n",
       "      <td>report</td>\n",
       "      <td>0.308153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_index         word     score\n",
       "907          907         stmt  1.000000\n",
       "3021        3021         bill  0.761461\n",
       "4861        4861    statement  0.739249\n",
       "8236        8236        bills  0.520205\n",
       "8635        8635   statements  0.465498\n",
       "3661        3661       letter  0.393868\n",
       "6094        6094  legislation  0.375259\n",
       "2552        2552          act  0.331694\n",
       "2928        2928         mark  0.313210\n",
       "3189        3189       report  0.308153"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('stmt',top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d3ce97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:43:48.853321Z",
     "start_time": "2022-10-01T11:43:45.019606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_index</th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>905</td>\n",
       "      <td>ftp</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5371</th>\n",
       "      <td>5371</td>\n",
       "      <td>file</td>\n",
       "      <td>0.691518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4651</th>\n",
       "      <td>4651</td>\n",
       "      <td>transfer</td>\n",
       "      <td>0.664068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8778</th>\n",
       "      <td>8778</td>\n",
       "      <td>protocol</td>\n",
       "      <td>0.572106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6764</th>\n",
       "      <td>6764</td>\n",
       "      <td>files</td>\n",
       "      <td>0.536686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_index      word     score\n",
       "905          905       ftp  1.000000\n",
       "5371        5371      file  0.691518\n",
       "4651        4651  transfer  0.664068\n",
       "8778        8778  protocol  0.572106\n",
       "6764        6764     files  0.536686"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('ftp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfcfe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:03:40.683084Z",
     "start_time": "2022-10-01T11:03:40.668167Z"
    }
   },
   "source": [
    "<b> Inference: </b> From the above table we can infer that the new words added have a proper vector representation and are close to similar meaning words."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a56c1089",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-24T07:25:18.699273Z",
     "start_time": "2022-09-24T07:25:18.629841Z"
    }
   },
   "source": [
    "#renaming vocab file \n",
    "\n",
    "os.chdir('./sentence-transformer-paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#renaming old vocab file\n",
    "!ren vocab.txt vocab_old.txt\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eb75be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:05.448058Z",
     "start_time": "2022-10-01T11:44:05.141564Z"
    }
   },
   "outputs": [],
   "source": [
    "#Updating the tokenizer\n",
    "with open('./sentence-transformer-paraphrase-MiniLM-L6-v2/tokenizer.json','r',encoding='utf-8') as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "    \n",
    "tokenizer_json['model']['vocab']=dict(sorted(vocab_wrd2idx.items(),key=lambda x:x[1]))\n",
    "\n",
    "#renaming vocab file \n",
    "\n",
    "os.chdir('./sentence-transformer-paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#renaming old vocab file\n",
    "!ren tokenizer.json tokenizer_old.json\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "with open('./sentence-transformer-paraphrase-MiniLM-L6-v2/tokenizer.json','w',encoding='utf-8') as f:\n",
    "    json.dump(tokenizer_json,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbde0739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:06.014530Z",
     "start_time": "2022-10-01T11:44:05.936958Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_tokenizer = AutoTokenizer.from_pretrained('./sentence-transformer-paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a128054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-25T07:10:43.257716Z",
     "start_time": "2022-09-25T07:10:43.234713Z"
    }
   },
   "source": [
    "<h3> Sample 1: </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761dd83",
   "metadata": {},
   "source": [
    "<h6> With Old Tokenizer without the word stmt added </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "475ab1da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:09.101228Z",
     "start_time": "2022-10-01T11:44:08.292453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37614718079566956"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'the statement is incorrect'\n",
    "text2 = 'the stmt is wrong'\n",
    "\n",
    "vector1 = encode(tokenizer,text1)\n",
    "vector2 = encode(tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012af127",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:07:21.628429Z",
     "start_time": "2022-10-01T11:07:21.618422Z"
    }
   },
   "source": [
    "<h6> With Updated Tokenizer containing the word stmt</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e56bdce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:09.780653Z",
     "start_time": "2022-10-01T11:44:09.350013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7418214082717896"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'the statement is incorrect'\n",
    "text2 = 'the stmt is wrong'\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f2c03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-25T07:12:09.176766Z",
     "start_time": "2022-09-25T07:12:09.165768Z"
    }
   },
   "source": [
    "<b> Result:</b> From the above Example we observe a signficant improvement on Similarity Score,as the model now has an understanding of the word stmt,Earlier the old Tokenizer was breaking the word <i>\"stmt\"</i> into <i>'st'</i> and <i>'##mt'</i> as it couldn't find stmt in the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801ca34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:16:08.011478Z",
     "start_time": "2022-10-01T11:16:08.001011Z"
    }
   },
   "source": [
    "<h3> Sample 2: </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51bade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:17:10.504023Z",
     "start_time": "2022-10-01T11:17:10.482293Z"
    }
   },
   "source": [
    "<h6> With Old Tokenizer without the word ftp added </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "895c2f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:11.690224Z",
     "start_time": "2022-10-01T11:44:11.303194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6022258400917053"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'unable to upload the data through ftp'\n",
    "text2 = 'file transfer protocol upload is not working'\n",
    "\n",
    "vector1 = encode(tokenizer,text1)\n",
    "vector2 = encode(tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e972da4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:17:50.274989Z",
     "start_time": "2022-10-01T11:17:50.252856Z"
    }
   },
   "source": [
    "<h6> With Updated Tokenizer containing the word ftp</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf82baa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:13.696613Z",
     "start_time": "2022-10-01T11:44:13.303197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.766219437122345"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'unable to upload the data through ftp'\n",
    "text2 = 'file transfer protocol upload is not working'\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61356dfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:16:44.164671Z",
     "start_time": "2022-10-01T11:16:44.144991Z"
    }
   },
   "source": [
    "<b> Result:</b> In this case aswell, we can observe a good increase in Similarity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740d150",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:23:32.896163Z",
     "start_time": "2022-10-01T11:23:32.874256Z"
    }
   },
   "source": [
    "<b>Conclutions:</b> \n",
    "    <ul>\n",
    "        <li>We Learnt to add new words to the existing vocab of a pretrained model </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbdcc54",
   "metadata": {},
   "source": [
    "<h3>2. Modify Existing word embeddings to adapt it for our use case </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84dba6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T07:05:25.825481Z",
     "start_time": "2022-08-31T07:05:25.809508Z"
    }
   },
   "source": [
    "Based on your use case same word can have different meaning in different context altough transformers are capable of handling such scenarios but alot of places:\n",
    "\n",
    "<ul>\n",
    "    <li>It wouldn't have that much context to work with</li>\n",
    "    <li>The degree to which it's considering the closeness is not statisfactory</li>\n",
    "</ul>\n",
    "\n",
    "That's why fine-tuning is required on the data you are working with , but if you are not able to do it due to any reason , the following can help.\n",
    "\n",
    "<p> Eg: In Finance dataset , statements and bills are used synonymously, but since you are using a pretrained model\n",
    "    there is high possiblity that your model is not considering it to a degree which you would it to</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38660afa",
   "metadata": {},
   "source": [
    "Let's understand the vector closeness between statement and bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4acc6268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:19.641919Z",
     "start_time": "2022-10-01T11:44:15.915030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_index</th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>4861</td>\n",
       "      <td>statement</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>907</td>\n",
       "      <td>stmt</td>\n",
       "      <td>0.739249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>8635</td>\n",
       "      <td>statements</td>\n",
       "      <td>0.635414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15974</th>\n",
       "      <td>15974</td>\n",
       "      <td>spokesperson</td>\n",
       "      <td>0.359564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7615</th>\n",
       "      <td>7615</td>\n",
       "      <td>comment</td>\n",
       "      <td>0.358950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8874</th>\n",
       "      <td>8874</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.354991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>14056</td>\n",
       "      <td>spokesman</td>\n",
       "      <td>0.353516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>3661</td>\n",
       "      <td>letter</td>\n",
       "      <td>0.352468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8170</th>\n",
       "      <td>8170</td>\n",
       "      <td>declaration</td>\n",
       "      <td>0.337585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12629</th>\n",
       "      <td>12629</td>\n",
       "      <td>remarks</td>\n",
       "      <td>0.329412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_index          word     score\n",
       "4861         4861     statement  1.000000\n",
       "907           907          stmt  0.739249\n",
       "8635         8635    statements  0.635414\n",
       "15974       15974  spokesperson  0.359564\n",
       "7615         7615       comment  0.358950\n",
       "8874         8874  announcement  0.354991\n",
       "14056       14056     spokesman  0.353516\n",
       "3661         3661        letter  0.352468\n",
       "8170         8170   declaration  0.337585\n",
       "12629       12629       remarks  0.329412"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('statement',top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e025157",
   "metadata": {},
   "source": [
    "From the above output we see that the general context for the word \"statement\" is considered as the data distribution of the corpus used broad,so a general model of language is attained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53d2695e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:19.665982Z",
     "start_time": "2022-10-01T11:44:19.646070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1263842135667801"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vocab_weights[vocab_wrd2idx['statement']],vocab_weights[vocab_wrd2idx['bill']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027e578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T07:36:42.685428Z",
     "start_time": "2022-08-31T07:36:42.669832Z"
    }
   },
   "source": [
    "From the above result, we can see that the similarity between the word \"statement\" and \"bill\" is very less than we would like it to be,\n",
    "now with transformer model this comparsion technqiue is not compeletly accurate , as we are just taking the embeddings from the first layer where no context modeling is done.\n",
    "\n",
    "Let's look at the similarity between encoded sentences with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c20ffb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:20.090036Z",
     "start_time": "2022-10-01T11:44:19.671595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7034520506858826"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'please send me the statement'\n",
    "text2 = 'please transfer the statement'\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec4bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T07:38:56.327499Z",
     "start_time": "2022-08-31T07:38:56.311522Z"
    }
   },
   "source": [
    "Both sentences are similar , and we are getting a similarity score of 0.70 which is somewhat reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc611847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:20.544362Z",
     "start_time": "2022-10-01T11:44:20.095764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3917175233364105"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'please send me the bill'\n",
    "text2 = 'please transfer the statement'\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1dda0",
   "metadata": {},
   "source": [
    "Now with statement replaced in once sentences the score is dropping by alot, ideally we would like it to be greater than 50% atleast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbdd4c",
   "metadata": {},
   "source": [
    "<h3> Let's understand the role of context here</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd75a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T07:45:53.286874Z",
     "start_time": "2022-08-31T07:45:53.269706Z"
    }
   },
   "source": [
    "<b> Without proper context </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "531c518c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:21.001507Z",
     "start_time": "2022-10-01T11:44:20.550376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3873918354511261"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'i did not recieve any bill' #from the bank\n",
    "text2 = \"i did'nt got any statement\" #from the bank\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d665f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T07:46:50.206117Z",
     "start_time": "2022-08-31T07:46:50.185887Z"
    }
   },
   "source": [
    "<b> Result: </b> Similarity Score is very low as general meaning of bill is considered here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b936d3b",
   "metadata": {},
   "source": [
    "<b> With Added Context </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5506ca22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:21.985187Z",
     "start_time": "2022-10-01T11:44:21.588024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6437106728553772"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'i did not recieve any bill from the bank'\n",
    "text2 = \"i did'nt got any statement from the bank\"\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8076614",
   "metadata": {},
   "source": [
    "<b>Result:</b> just by adding \"from the bank\" , we can observe a significant improvement from 0.38 to 0.64 in the similarity score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b711e8",
   "metadata": {},
   "source": [
    "But while working with the data it's not neccessary that a proper context will be always present, let's go back to our prev example and try to modify the embeddings to get reasonable similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d36d8946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:23.136360Z",
     "start_time": "2022-10-01T11:44:23.116475Z"
    }
   },
   "outputs": [],
   "source": [
    "#replacing statement embeddings with avg of both 'statement' and 'bill'\n",
    "vocab_weights[vocab_wrd2idx['statement']] = (vocab_weights[vocab_wrd2idx['statement']] + vocab_weights[vocab_wrd2idx['bill']])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99fe33ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:23.606293Z",
     "start_time": "2022-10-01T11:44:23.432079Z"
    }
   },
   "outputs": [],
   "source": [
    "#settings weights\n",
    "model_weights[0] = vocab_weights\n",
    "model.set_weights(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "667aa76f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:27.562218Z",
     "start_time": "2022-10-01T11:44:23.811233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_index</th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>907</td>\n",
       "      <td>stmt</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>4861</td>\n",
       "      <td>statement</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>3021</td>\n",
       "      <td>bill</td>\n",
       "      <td>0.761461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8236</th>\n",
       "      <td>8236</td>\n",
       "      <td>bills</td>\n",
       "      <td>0.520205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>8635</td>\n",
       "      <td>statements</td>\n",
       "      <td>0.465498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>3661</td>\n",
       "      <td>letter</td>\n",
       "      <td>0.393868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>6094</td>\n",
       "      <td>legislation</td>\n",
       "      <td>0.375259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>2552</td>\n",
       "      <td>act</td>\n",
       "      <td>0.331694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>2928</td>\n",
       "      <td>mark</td>\n",
       "      <td>0.313210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>3189</td>\n",
       "      <td>report</td>\n",
       "      <td>0.308153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_index         word     score\n",
       "907          907         stmt  1.000000\n",
       "4861        4861    statement  1.000000\n",
       "3021        3021         bill  0.761461\n",
       "8236        8236        bills  0.520205\n",
       "8635        8635   statements  0.465498\n",
       "3661        3661       letter  0.393868\n",
       "6094        6094  legislation  0.375259\n",
       "2552        2552          act  0.331694\n",
       "2928        2928         mark  0.313210\n",
       "3189        3189       report  0.308153"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('statement',top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97211d7a",
   "metadata": {},
   "source": [
    "<b>Result</b>: From the above result we can observe that now bill and statement vectors are nearby in the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "880904b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:44:28.017569Z",
     "start_time": "2022-10-01T11:44:27.569129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5899935364723206"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'please send me the bill'\n",
    "text2 = 'please transfer the statement'\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4c610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T08:11:09.871443Z",
     "start_time": "2022-08-31T08:11:09.852809Z"
    }
   },
   "source": [
    "<b>Result:</b> Now with the embeddings modified we see a significant jump in similarity score from 0.39 to 0.58 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe126c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:38:32.978318Z",
     "start_time": "2022-10-01T11:38:32.949955Z"
    }
   },
   "source": [
    "<p>\n",
    "    <b> Conclusion : From the above experiments </b>\n",
    "    <ul>\n",
    "        <li>We learnt how we can add and modify the vectors manually to Increase the model Performance</li>\n",
    "        <li>We understood the model Artichtecture, and understood why there are so many [unused0] tokens available in the vocabulary</li>\n",
    "        <li>Built Functions like:\n",
    "            <ul>\n",
    "                <li><i>most_similar</i> to understand the words similarity in vector space </li>\n",
    "                <li><i>add_words,weighted_average_of_vectors</i> to add new words to our vocab</li>\n",
    "            </ul>\n",
    "        <li> Understood the importance of context for Embeddings</li>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
