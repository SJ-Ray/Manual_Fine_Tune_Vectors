{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ecb775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T06:38:07.683459Z",
     "start_time": "2022-08-31T06:38:07.674401Z"
    }
   },
   "source": [
    "<h2> Fine Tune Sentence Transformer Embeddings without Model Training </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9045231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:52:42.453121Z",
     "start_time": "2022-10-01T11:52:30.157101Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e5bc2",
   "metadata": {},
   "source": [
    "<b> Sentence Transformers:</b> <p>Sentence Transformers is a Python framework for state-of-the-art sentence, text embeddings. It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like semantic searchÂ , paraphrase mining.</p>\n",
    "\n",
    "<b> Paraphrase-MiniLM-L6-v2:</b> <p> is based on BERT with 6 Transformer Encoder Layers,it can handle 512 tokens and return dense vector representation with 384 features</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399056d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:52:54.859589Z",
     "start_time": "2022-10-01T11:52:54.824171Z"
    }
   },
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = tf.cast(tf.tile(tf.expand_dims(attention_mask, -1), [1, 1, token_embeddings.shape[-1]]), tf.float32)\n",
    "    return tf.math.reduce_sum(token_embeddings * input_mask_expanded, 1) / tf.math.maximum(tf.math.reduce_sum(input_mask_expanded, 1), 1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(tokenizer,texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
    "    \n",
    "    # Compute token embeddings\n",
    "    model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = tf.math.l2_normalize(embeddings, axis=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#cosine similarity Function\n",
    "def cosine_similarity(vector1,vector2):\n",
    "    return (1- cosine(vector1,vector2))\n",
    "\n",
    "#Function to find top_k similar words\n",
    "def most_similar(search_word,top_k=5):\n",
    "    search_idx = vocab_wrd2idx[search_word]\n",
    "    similarity_ls=[]\n",
    "    for word_idx,word_embed in enumerate(vocab_weights):\n",
    "        similarity_score = cosine_similarity(vocab_weights[search_idx],word_embed)\n",
    "        similarity_ls.append((word_idx,vocab_idx2wrd[word_idx],similarity_score))\n",
    "    \n",
    "    return pd.DataFrame(similarity_ls,columns=['word_index','word','score']).sort_values(by='score',ascending=False)[0:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15cc804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:01.783289Z",
     "start_time": "2022-10-01T11:52:55.600383Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ./sentence-transformer-paraphrase-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./sentence-transformer-paraphrase-MiniLM-L6-v2')\n",
    "model = TFAutoModel.from_pretrained('./sentence-transformer-paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a2cad7",
   "metadata": {},
   "source": [
    "<b> Model Architecture</b>\n",
    "\n",
    "<div style=\"overflow-y: scroll; height:400px;\">\n",
    "<pre>\n",
    "BertModel(\n",
    "  (embeddings): BertEmbeddings(\n",
    "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
    "    (position_embeddings): Embedding(512, 384)\n",
    "    (token_type_embeddings): Embedding(2, 384)\n",
    "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "    (dropout): Dropout(p=0.1, inplace=False)\n",
    "  )\n",
    "  (encoder): BertEncoder(\n",
    "    (layer): ModuleList(\n",
    "      (0): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (1): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (2): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (3): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (4): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (5): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
    "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (pooler): BertPooler(\n",
    "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
    "    (activation): Tanh()\n",
    "  )\n",
    ")\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44091072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T06:50:27.109337Z",
     "start_time": "2022-08-31T06:50:27.094311Z"
    }
   },
   "source": [
    "<h3> Objective: </h3>\n",
    "<ul>\n",
    "    <li> Add New Words to Vocabulary </li>\n",
    "    <li> Modify Existing word embeddings to adapt it for our use case</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639d5ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T06:52:41.248425Z",
     "start_time": "2022-08-31T06:52:41.235343Z"
    }
   },
   "source": [
    "<h2>1. Add New Words to Vocabulary </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db8de4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T06:45:11.492096Z",
     "start_time": "2022-08-31T06:45:11.469316Z"
    }
   },
   "source": [
    "<h4> Vocabulary Exploration </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2797a3c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:01.891266Z",
     "start_time": "2022-10-01T11:53:01.788425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('[PAD]', 0),\n",
       " ('[unused0]', 1),\n",
       " ('[unused1]', 2),\n",
       " ('[unused2]', 3),\n",
       " ('[unused3]', 4),\n",
       " ('[unused4]', 5),\n",
       " ('[unused5]', 6),\n",
       " ('[unused6]', 7),\n",
       " ('[unused7]', 8),\n",
       " ('[unused8]', 9)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_wrd2idx = tokenizer.vocab\n",
    "vocab_idx2wrd = {v:k for k,v in vocab_wrd2idx.items()}\n",
    "\n",
    "print(len(vocab_wrd2idx))\n",
    "\n",
    "sorted(vocab_wrd2idx.items(),key=lambda x:x[1])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11364c86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T06:50:49.654741Z",
     "start_time": "2022-08-31T06:50:49.627778Z"
    }
   },
   "source": [
    "<p> We have 30,522 tokens in our vocab for paraphrase-MiniLM-l6-v2 model.\n",
    "It has 993 tokens which are not used, represented by [unusedXXX], \n",
    "These are the token which we can replace to add new tokens to our vocab.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92a03fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:02.129212Z",
     "start_time": "2022-10-01T11:53:01.895940Z"
    }
   },
   "outputs": [],
   "source": [
    "#extracting model weights\n",
    "model_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe77b7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:02.144477Z",
     "start_time": "2022-10-01T11:53:02.134871Z"
    }
   },
   "outputs": [],
   "source": [
    "#getting vocab weights\n",
    "vocab_weights = model_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af736d6a",
   "metadata": {},
   "source": [
    "<p>\n",
    "Suppose you have trained your model or you are using a pretrained model but what you observe is that some important words are missing from the vocabulary, and you don't want to retrain the model with new data,\n",
    "you can follow the below method to add new word/tokens to your model\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f028aa",
   "metadata": {},
   "source": [
    "<h4> Code for adding words </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ccb7ed1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:03.575472Z",
     "start_time": "2022-10-01T11:53:03.551463Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_average_of_vectors(word_wghts):\n",
    "    \n",
    "    vector = np.zeros(384, dtype=np.float64)\n",
    "    \n",
    "    for key,value in word_wghts.items():\n",
    "        vector += (1 - value) * vocab_weights[vocab_wrd2idx[key]]\n",
    "    \n",
    "    vector = normalize(vector.reshape(1,-1))[0]\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def add_word(word_to_replace,word_to_add,word_wghts):\n",
    "    \n",
    "    #editing the dictionary \n",
    "    vocab_index = vocab_wrd2idx[word_to_replace]\n",
    "    vocab_wrd2idx[word_to_add] = vocab_index\n",
    "    del vocab_wrd2idx[vocab_idx2wrd[vocab_index]]\n",
    "    vocab_idx2wrd[vocab_index] = word_to_add\n",
    "    \n",
    "    #add the vector\n",
    "    vocab_weights[vocab_index] = weighted_average_of_vectors(word_wghts)\n",
    "    \n",
    "    #settings weights\n",
    "    model_weights[0] = vocab_weights\n",
    "    model.set_weights(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69d609b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:04.246799Z",
     "start_time": "2022-10-01T11:53:04.147380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, True]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if words are in vocab\n",
    "\n",
    "words_to_add = [\"stmt\",\"ftp\",'http']\n",
    "\n",
    "[wrd in tokenizer.vocab for wrd in words_to_add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433b2da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T09:50:22.559033Z",
     "start_time": "2022-08-31T09:50:22.548236Z"
    }
   },
   "source": [
    "<b>Result:</b> only http is present in the vocab. ftp and stmt (short for stament) is not present (you can always replace the word at preprocessing,this is just for demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad8862e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:11.888512Z",
     "start_time": "2022-10-01T11:53:11.560405Z"
    }
   },
   "outputs": [],
   "source": [
    "#defining weights for new words\n",
    "\n",
    "ftp_wghts = {\n",
    "    'file':0.3,\n",
    "    'transfer':0.3,\n",
    "    'protocol':0.4,\n",
    "}\n",
    "\n",
    "stmt_wghts = {\n",
    "    'statement':0.5,\n",
    "    'bill':0.5\n",
    "}\n",
    "\n",
    "#adding words\n",
    "add_word('[unused900]','ftp',ftp_wghts)\n",
    "add_word('[unused902]','stmt',stmt_wghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3617e7eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:16.286549Z",
     "start_time": "2022-10-01T11:53:12.411999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_index</th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>907</td>\n",
       "      <td>stmt</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>3021</td>\n",
       "      <td>bill</td>\n",
       "      <td>0.761461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>4861</td>\n",
       "      <td>statement</td>\n",
       "      <td>0.739249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8236</th>\n",
       "      <td>8236</td>\n",
       "      <td>bills</td>\n",
       "      <td>0.520205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>8635</td>\n",
       "      <td>statements</td>\n",
       "      <td>0.465498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>3661</td>\n",
       "      <td>letter</td>\n",
       "      <td>0.393868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>6094</td>\n",
       "      <td>legislation</td>\n",
       "      <td>0.375259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>2552</td>\n",
       "      <td>act</td>\n",
       "      <td>0.331694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>2928</td>\n",
       "      <td>mark</td>\n",
       "      <td>0.313210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>3189</td>\n",
       "      <td>report</td>\n",
       "      <td>0.308153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_index         word     score\n",
       "907          907         stmt  1.000000\n",
       "3021        3021         bill  0.761461\n",
       "4861        4861    statement  0.739249\n",
       "8236        8236        bills  0.520205\n",
       "8635        8635   statements  0.465498\n",
       "3661        3661       letter  0.393868\n",
       "6094        6094  legislation  0.375259\n",
       "2552        2552          act  0.331694\n",
       "2928        2928         mark  0.313210\n",
       "3189        3189       report  0.308153"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('stmt',top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d3ce97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:20.062856Z",
     "start_time": "2022-10-01T11:53:16.291398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_index</th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>905</td>\n",
       "      <td>ftp</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5371</th>\n",
       "      <td>5371</td>\n",
       "      <td>file</td>\n",
       "      <td>0.691518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4651</th>\n",
       "      <td>4651</td>\n",
       "      <td>transfer</td>\n",
       "      <td>0.664068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8778</th>\n",
       "      <td>8778</td>\n",
       "      <td>protocol</td>\n",
       "      <td>0.572106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6764</th>\n",
       "      <td>6764</td>\n",
       "      <td>files</td>\n",
       "      <td>0.536686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_index      word     score\n",
       "905          905       ftp  1.000000\n",
       "5371        5371      file  0.691518\n",
       "4651        4651  transfer  0.664068\n",
       "8778        8778  protocol  0.572106\n",
       "6764        6764     files  0.536686"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('ftp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02eb75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:03:40.683084Z",
     "start_time": "2022-10-01T11:03:40.668167Z"
    }
   },
   "source": [
    "<b> Inference: </b> From the above table we can infer that the new words added have a proper vector representation and are close to similar meaning words."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a56c1089",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-24T07:25:18.699273Z",
     "start_time": "2022-09-24T07:25:18.629841Z"
    }
   },
   "source": [
    "#renaming vocab file \n",
    "\n",
    "os.chdir('./sentence-transformer-paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#renaming old vocab file\n",
    "!ren vocab.txt vocab_old.txt\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eb75be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:20.366383Z",
     "start_time": "2022-10-01T11:53:20.068050Z"
    }
   },
   "outputs": [],
   "source": [
    "#Updating the tokenizer\n",
    "with open('./sentence-transformer-paraphrase-MiniLM-L6-v2/tokenizer.json','r',encoding='utf-8') as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "    \n",
    "tokenizer_json['model']['vocab']=dict(sorted(vocab_wrd2idx.items(),key=lambda x:x[1]))\n",
    "\n",
    "#renaming vocab file \n",
    "\n",
    "os.chdir('./sentence-transformer-paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#renaming old vocab file\n",
    "!ren tokenizer.json tokenizer_old.json\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "with open('./sentence-transformer-paraphrase-MiniLM-L6-v2/tokenizer.json','w',encoding='utf-8') as f:\n",
    "    json.dump(tokenizer_json,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbde0739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:20.447320Z",
     "start_time": "2022-10-01T11:53:20.371448Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_tokenizer = AutoTokenizer.from_pretrained('./sentence-transformer-paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a128054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-25T07:10:43.257716Z",
     "start_time": "2022-09-25T07:10:43.234713Z"
    }
   },
   "source": [
    "<h3> Sample 1: </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616372b3",
   "metadata": {},
   "source": [
    "<h6> With Old Tokenizer without the word stmt added </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "475ab1da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:20.926028Z",
     "start_time": "2022-10-01T11:53:20.452953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37614718079566956"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'the statement is incorrect'\n",
    "text2 = 'the stmt is wrong'\n",
    "\n",
    "vector1 = encode(tokenizer,text1)\n",
    "vector2 = encode(tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a9bcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:07:21.628429Z",
     "start_time": "2022-10-01T11:07:21.618422Z"
    }
   },
   "source": [
    "<h6> With Updated Tokenizer containing the word stmt</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63d6b50c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:21.781825Z",
     "start_time": "2022-10-01T11:53:21.369200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7418214082717896"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'the statement is incorrect'\n",
    "text2 = 'the stmt is wrong'\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f2c03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-25T07:12:09.176766Z",
     "start_time": "2022-09-25T07:12:09.165768Z"
    }
   },
   "source": [
    "<b> Result:</b> From the above Example we observe a signficant improvement on Similarity Score,as the model now has an understanding of the word stmt,Earlier the old Tokenizer was breaking the word <i>\"stmt\"</i> into <i>'st'</i> and <i>'##mt'</i> as it couldn't find stmt in the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad36bcea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:16:08.011478Z",
     "start_time": "2022-10-01T11:16:08.001011Z"
    }
   },
   "source": [
    "<h3> Sample 2: </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab1f84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:17:10.504023Z",
     "start_time": "2022-10-01T11:17:10.482293Z"
    }
   },
   "source": [
    "<h6> With Old Tokenizer without the word ftp added </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895c2f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:23.706564Z",
     "start_time": "2022-10-01T11:53:23.281199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6022258400917053"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'unable to upload the data through ftp'\n",
    "text2 = 'file transfer protocol upload is not working'\n",
    "\n",
    "vector1 = encode(tokenizer,text1)\n",
    "vector2 = encode(tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830cfbe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:17:50.274989Z",
     "start_time": "2022-10-01T11:17:50.252856Z"
    }
   },
   "source": [
    "<h6> With Updated Tokenizer containing the word ftp</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf82baa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:24.823304Z",
     "start_time": "2022-10-01T11:53:24.412474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.766219437122345"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'unable to upload the data through ftp'\n",
    "text2 = 'file transfer protocol upload is not working'\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d914478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:16:44.164671Z",
     "start_time": "2022-10-01T11:16:44.144991Z"
    }
   },
   "source": [
    "<b> Result:</b> In this case aswell, we can observe a good increase in Similarity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c5b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:23:32.896163Z",
     "start_time": "2022-10-01T11:23:32.874256Z"
    }
   },
   "source": [
    "<b>Conclutions:</b> \n",
    "    <ul>\n",
    "        <li>We Learnt to add new words to the existing vocab of a pretrained model </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbdcc54",
   "metadata": {},
   "source": [
    "<h3>2. Modify Existing word embeddings to adapt it for our use case </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84dba6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T07:05:25.825481Z",
     "start_time": "2022-08-31T07:05:25.809508Z"
    }
   },
   "source": [
    "Based on your use case same word can have different meaning in different context altough transformers are capable of handling such scenarios but alot of places:\n",
    "\n",
    "<ul>\n",
    "    <li>It wouldn't have that much context to work with</li>\n",
    "    <li>The degree to which it's considering the closeness is not statisfactory</li>\n",
    "</ul>\n",
    "\n",
    "That's why fine-tuning is required on the data you are working with , but if you are not able to do it due to any reason , the following can help.\n",
    "\n",
    "<p> Eg: In Finance dataset , statements and bills are used synonymously, but since you are using a pretrained model\n",
    "    there is high possiblity that your model is not considering it to a degree which you would it to</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38660afa",
   "metadata": {},
   "source": [
    "Let's understand the vector closeness between statement and bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4acc6268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:30.732165Z",
     "start_time": "2022-10-01T11:53:27.043286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_index</th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>4861</td>\n",
       "      <td>statement</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>907</td>\n",
       "      <td>stmt</td>\n",
       "      <td>0.739249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>8635</td>\n",
       "      <td>statements</td>\n",
       "      <td>0.635414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15974</th>\n",
       "      <td>15974</td>\n",
       "      <td>spokesperson</td>\n",
       "      <td>0.359564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7615</th>\n",
       "      <td>7615</td>\n",
       "      <td>comment</td>\n",
       "      <td>0.358950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8874</th>\n",
       "      <td>8874</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.354991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>14056</td>\n",
       "      <td>spokesman</td>\n",
       "      <td>0.353516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>3661</td>\n",
       "      <td>letter</td>\n",
       "      <td>0.352468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8170</th>\n",
       "      <td>8170</td>\n",
       "      <td>declaration</td>\n",
       "      <td>0.337585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12629</th>\n",
       "      <td>12629</td>\n",
       "      <td>remarks</td>\n",
       "      <td>0.329412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_index          word     score\n",
       "4861         4861     statement  1.000000\n",
       "907           907          stmt  0.739249\n",
       "8635         8635    statements  0.635414\n",
       "15974       15974  spokesperson  0.359564\n",
       "7615         7615       comment  0.358950\n",
       "8874         8874  announcement  0.354991\n",
       "14056       14056     spokesman  0.353516\n",
       "3661         3661        letter  0.352468\n",
       "8170         8170   declaration  0.337585\n",
       "12629       12629       remarks  0.329412"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('statement',top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e025157",
   "metadata": {},
   "source": [
    "From the above output we see that the general context for the word \"statement\" is considered as the data distribution of the corpus used broad,so a general model of language is attained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53d2695e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:30.755285Z",
     "start_time": "2022-10-01T11:53:30.737994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1263842135667801"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vocab_weights[vocab_wrd2idx['statement']],vocab_weights[vocab_wrd2idx['bill']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027e578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T07:36:42.685428Z",
     "start_time": "2022-08-31T07:36:42.669832Z"
    }
   },
   "source": [
    "From the above result, we can see that the similarity between the word \"statement\" and \"bill\" is very less than we would like it to be,\n",
    "now with transformer model this comparsion technqiue is not compeletly accurate , as we are just taking the embeddings from the first layer where no context modeling is done.\n",
    "\n",
    "Let's look at the similarity between encoded sentences with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c20ffb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:31.222482Z",
     "start_time": "2022-10-01T11:53:30.760731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7034520506858826"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'please send me the statement'\n",
    "text2 = 'please transfer the statement'\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec4bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T07:38:56.327499Z",
     "start_time": "2022-08-31T07:38:56.311522Z"
    }
   },
   "source": [
    "Both sentences are similar , and we are getting a similarity score of 0.70 which is somewhat reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc611847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:31.688959Z",
     "start_time": "2022-10-01T11:53:31.227205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3917175233364105"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'please send me the bill'\n",
    "text2 = 'please transfer the statement'\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1dda0",
   "metadata": {},
   "source": [
    "Now with statement replaced in once sentences the score is dropping by alot, ideally we would like it to be greater than 50% atleast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbdd4c",
   "metadata": {},
   "source": [
    "<h3> Let's understand the role of context here</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd75a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T07:45:53.286874Z",
     "start_time": "2022-08-31T07:45:53.269706Z"
    }
   },
   "source": [
    "<b> Without proper context </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "531c518c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:33.154261Z",
     "start_time": "2022-10-01T11:53:32.759144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3873918354511261"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'i did not recieve any bill' #from the bank\n",
    "text2 = \"i did'nt got any statement\" #from the bank\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d665f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T07:46:50.206117Z",
     "start_time": "2022-08-31T07:46:50.185887Z"
    }
   },
   "source": [
    "<b> Result: </b> Similarity Score is very low as general meaning of bill is considered here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b936d3b",
   "metadata": {},
   "source": [
    "<b> With Added Context </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5506ca22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:34.239125Z",
     "start_time": "2022-10-01T11:53:33.823551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6437106728553772"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'i did not recieve any bill from the bank'\n",
    "text2 = \"i did'nt got any statement from the bank\"\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8076614",
   "metadata": {},
   "source": [
    "<b>Result:</b> just by adding \"from the bank\" , we can observe a significant improvement from 0.38 to 0.64 in the similarity score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b711e8",
   "metadata": {},
   "source": [
    "But while working with the data it's not neccessary that a proper context will be always present, let's go back to our prev example and try to modify the embeddings to get reasonable similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d36d8946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:35.026807Z",
     "start_time": "2022-10-01T11:53:35.004239Z"
    }
   },
   "outputs": [],
   "source": [
    "#replacing statement embeddings with avg of both 'statement' and 'bill'\n",
    "vocab_weights[vocab_wrd2idx['statement']] = (vocab_weights[vocab_wrd2idx['statement']] + vocab_weights[vocab_wrd2idx['bill']])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99fe33ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:35.727249Z",
     "start_time": "2022-10-01T11:53:35.547332Z"
    }
   },
   "outputs": [],
   "source": [
    "#settings weights\n",
    "model_weights[0] = vocab_weights\n",
    "model.set_weights(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "667aa76f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:39.697642Z",
     "start_time": "2022-10-01T11:53:36.021832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_index</th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>907</td>\n",
       "      <td>stmt</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>4861</td>\n",
       "      <td>statement</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>3021</td>\n",
       "      <td>bill</td>\n",
       "      <td>0.761461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8236</th>\n",
       "      <td>8236</td>\n",
       "      <td>bills</td>\n",
       "      <td>0.520205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>8635</td>\n",
       "      <td>statements</td>\n",
       "      <td>0.465498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>3661</td>\n",
       "      <td>letter</td>\n",
       "      <td>0.393868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>6094</td>\n",
       "      <td>legislation</td>\n",
       "      <td>0.375259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>2552</td>\n",
       "      <td>act</td>\n",
       "      <td>0.331694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>2928</td>\n",
       "      <td>mark</td>\n",
       "      <td>0.313210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>3189</td>\n",
       "      <td>report</td>\n",
       "      <td>0.308153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_index         word     score\n",
       "907          907         stmt  1.000000\n",
       "4861        4861    statement  1.000000\n",
       "3021        3021         bill  0.761461\n",
       "8236        8236        bills  0.520205\n",
       "8635        8635   statements  0.465498\n",
       "3661        3661       letter  0.393868\n",
       "6094        6094  legislation  0.375259\n",
       "2552        2552          act  0.331694\n",
       "2928        2928         mark  0.313210\n",
       "3189        3189       report  0.308153"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('statement',top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97211d7a",
   "metadata": {},
   "source": [
    "<b>Result</b>: From the above result we can observe that now bill and statement vectors are nearby in the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "880904b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:53:40.156060Z",
     "start_time": "2022-10-01T11:53:39.702068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5899935364723206"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'please send me the bill'\n",
    "text2 = 'please transfer the statement'\n",
    "\n",
    "vector1 = encode(updated_tokenizer,text1)\n",
    "vector2 = encode(updated_tokenizer,text2)\n",
    "\n",
    "cosine_similarity(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4c610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T08:11:09.871443Z",
     "start_time": "2022-08-31T08:11:09.852809Z"
    }
   },
   "source": [
    "<b>Result:</b> Now with the embeddings modified we see a significant jump in similarity score from 0.39 to 0.58 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538ba3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-01T11:38:32.978318Z",
     "start_time": "2022-10-01T11:38:32.949955Z"
    }
   },
   "source": [
    "<p>\n",
    "    <b> Conclusion : From the above experiments </b>\n",
    "    <ul>\n",
    "        <li>We learnt how we can add and modify the vectors manually to Increase the model Performance</li>\n",
    "        <li>We understood the model Artichtecture, and understood why there are so many [unused0] tokens available in the vocabulary</li>\n",
    "        <li>Built Functions like:\n",
    "            <ul>\n",
    "                <li><i>most_similar</i> to understand the words similarity in vector space </li>\n",
    "                <li><i>add_words,weighted_average_of_vectors</i> to add new words to our vocab</li>\n",
    "            </ul>\n",
    "        <li> Understood the importance of context for Embeddings</li>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
